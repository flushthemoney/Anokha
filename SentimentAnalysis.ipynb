{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f681950-6fec-4141-b0f8-82cf4a13650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type\n",
      "0    1000\n",
      "1    1000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>1496</td>\n",
       "      <td>jugs wuzzie thirdeye bumfuck knife failure vir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>First primitive 1834, and the. Bike lanes wors...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>611</td>\n",
       "      <td>By 27,000 neuringer made a federal parliamenta...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>938</td>\n",
       "      <td>Abstraction of similar statistical. Role as er...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>572</td>\n",
       "      <td>Few seconds. s \\n \\n new york hosted the 2014....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                            content  type\n",
       "1496        1496  jugs wuzzie thirdeye bumfuck knife failure vir...     1\n",
       "270          270  First primitive 1834, and the. Bike lanes wors...     0\n",
       "611          611  By 27,000 neuringer made a federal parliamenta...     0\n",
       "938          938  Abstraction of similar statistical. Role as er...     0\n",
       "572          572  Few seconds. s \\n \\n new york hosted the 2014....     0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import transformers\n",
    "\n",
    "data = pd.read_csv(\"C:\\\\Users\\\\kunwa\\\\SIH\\\\Data2.csv\")\n",
    "data.head()\n",
    "\n",
    "print(data['type'].value_counts())\n",
    "data = data.sample(frac=1, random_state=32)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fd94ae82-cd09-40f7-8f49-c011b7184946",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1600, 400)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
    "import torch\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
    "\n",
    "print(model)\n",
    "\n",
    "# model = model.to('cuda')\n",
    "\n",
    "sample_data = [\"I am eating\",\"I am playing \"]\n",
    "tokenizer(sample_data, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "X = list(data[\"content\"])\n",
    "y = list(data[\"type\"])\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,stratify=y)\n",
    "X_train_tokenized = tokenizer(X_train, padding=True, truncation=True, max_length=512)\n",
    "X_val_tokenized = tokenizer(X_val, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "X_train_tokenized.keys()\n",
    "\n",
    "print(X_train_tokenized['attention_mask'][0])\n",
    "\n",
    "len(X_train),len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2dea3e-4869-4253-8b69-4b3c63eab939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "77a56830-beb6-4c00-abe5-d468f708e71d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([  101,  6616,  4168, 11783, 14163,  4246,  4305,  6299,  5413,  2243,\n",
      "         3348, 14971,  2213, 14163, 26091,  2099,  5980,  4830,  3995, 22212,\n",
      "        15454, 18082,  2121,  8444,  9152, 23033,  2480, 12581, 12170,  4017,\n",
      "         2818, 13970,  5244,  3786,  1011,  2125,  4954,  2497,  4485, 11263,\n",
      "         9102,  9767,  2388, 11263,  9102,  6616,  3085, 16477,  2229, 27307,\n",
      "        10722,  4103,  2009,  2818,  2643,  3286, 22930,  5413,  4801, 14910,\n",
      "         5017,  4485,  3334, 10007, 11263,  3600, 10338,  4974,  8670, 13150,\n",
      "         3022, 16405,  6508, 14841, 15353,  7455,  2243, 20878, 21146,  2102,\n",
      "        19724,  5280,  4904,  4344, 11263,  3600,  4429,  2243, 24894, 17219,\n",
      "        11263,  9102,  5968, 10007,  4632, 14289,  9397,  3111,  4485,  4974,\n",
      "        15385,  2121,  5762,  2377, 15239,  2067, 23835,  5506, 10398, 29393,\n",
      "        11106,  9890,  2093, 14045,  4424, 11865, 11818, 23947,  2121, 13988,\n",
      "         5017, 17540,  3347, 20961,  3401,  3062, 10450,  2080,  8872,  9869,\n",
      "        23618, 15385,  4168,  6097, 19629,  4314,  4735, 16405, 18719,  2063,\n",
      "         6583, 29566,  4588, 11460,  4590,  2053,  6559, 10147, 13793,  2213,\n",
      "        11721,  4263, 26068,  2102, 19215,  3059,  2080, 16031,  5017,  3096,\n",
      "        10258, 10421,  6616,  8197,  2290, 10147, 13213,  2819,  7570,  2094,\n",
      "        11239, 20910, 10958,  2620,  2015, 16709,  3367,  7781,  2121,  7743,\n",
      "         2100,  2417,  7138,  7388, 14301,  2121,  4485, 24066, 10556,  4246,\n",
      "         2890,  8917,  2050,  9152,  2290, 14163, 15379,  4305,  6299, 10147,\n",
      "         3501,  4478,  5092,  2080,  2647, 10147,  6491,  7279,  5149,  7087,\n",
      "         4305,  3489, 22889,  4904, 11937,  4246,  9152, 23033, 13988,  8569,\n",
      "        11362, 10577,  2121, 22889,  4402,  4371, 16078, 26352, 11263,  3600,\n",
      "         8579,  1011, 17219, 17153, 20689, 27377,  3070,  8700, 18072, 11602,\n",
      "        27263,  7088, 10695,  2100,  3280,  9573,  2100,  2822,  2190,  4818,\n",
      "         1055, 25518,  4590,  2075, 12290,  7109, 10458, 21025,  9397,  7916,\n",
      "         5302,  2863, 14163,  4246, 18393, 17119,  8867,  2190,  4818,  3012,\n",
      "        17996,  5980,  3238,  2827, 14298,  4892,  3393, 12036,  6616,  2378,\n",
      "        15950, 13988,  4226,  2368,  4632,  3597,  2860, 11097,  4344, 14876,\n",
      "         7716, 23264,  2290, 13970,  2213, 16211,  2102,  2176,  2102, 12449,\n",
      "         3723,  4248,  2666,  5472,  2213, 16510, 21354,  4632,  8202, 14839,\n",
      "         3348,  2860, 16892,  6065,  2099,  2128,  2890,  5636, 10147,  3654,\n",
      "        23198,  6909,  9302, 19739, 22414, 11529,  9744,  2507,  4974, 16405,\n",
      "        11796, 18098,  2594,  2791,  5980, 25230,  2121,  2061,  9527, 11865,\n",
      "         9468,  2243, 22855,  7162, 16709, 23809,  1055, 25518, 15776, 25283,\n",
      "         8034,  6472, 18047,  5886,  4063, 10768, 20051,  3695, 11891,  8029,\n",
      "        12054,  2317,  2100, 11586,  2243,  7493,  5820, 10354, 12722,  2887,\n",
      "        22418, 14301,  2121,  6616,  8202, 14839,  9152,  2361, 18391,  3040,\n",
      "        20179,  8042, 22418,  5243,  3334,  6080, 25153,  4845,  6997, 13433,\n",
      "         2080,  4521,  4168, 14255,  8737,  2121,  2040,  2361, 28629,  5558,\n",
      "        29183,  6904, 13871,  4140, 14841,  2102,  5558,  2497, 18758, 25683,\n",
      "        14841, 19646,  7840,  7395,  8684, 22129,  4632, 14301,  2121, 16405,\n",
      "         4757,  5515, 12731, 10695,  6530,  2784,  2705,  3217,  4017, 11113,\n",
      "         5092, 18047, 13988, 14081,  3532,  2860, 16584,  3388,  8180,  2232,\n",
      "        14123,  2100, 16776, 28394,  3329, 14117,  2053, 11263, 23177,  4576,\n",
      "         6033,  2227, 11263,  9102, 10556, 26989,  2099, 10823,  3608, 25230,\n",
      "         2121,  6887,  6968, 11829,  4590, 12731,  3372, 11263,  9102,  5572,\n",
      "         2102, 24053,  8950,  2100,  2860, 16892,  9152, 23033, 25547,  2094,\n",
      "         4632, 12707,  6616,  7559,  2094, 11640, 10288, 12934,  2100,  8233,\n",
      "         3723,  8197,  8737,  8917,  2100, 14255,  8737,  9103,  6610, 20277,\n",
      "         2480,  2919, 11263,  3600,  4632, 25230,  2121,  2388, 11263, 23177,\n",
      "        13186,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1]), 'labels': tensor(1)}\n"
     ]
    }
   ],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels=None):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        # print(self.labels)\n",
    "        if self.labels:\n",
    "            item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings[\"input_ids\"])\n",
    "\n",
    "train_dataset = Dataset(X_train_tokenized, y_train)\n",
    "val_dataset = Dataset(X_val_tokenized, y_val)\n",
    "\n",
    "print(train_dataset[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a86b60e7-7106-49f9-a7bf-85d9a9d2962f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    # print(type(p))\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred)\n",
    "    precision = precision_score(y_true=labels, y_pred=pred)\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03c05f9b-70f9-443b-9435-5aa660ca4cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 34:07, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.2426805877685547, metrics={'train_runtime': 2056.332, 'train_samples_per_second': 0.778, 'train_steps_per_second': 0.097, 'total_flos': 420977688576000.0, 'train_loss': 0.2426805877685547, 'epoch': 1.0})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define Trainer\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8\n",
    "\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f168b5ab-e208-4fb9-bda9-374d097aa77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 04:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.23287808895111084,\n",
       " 'eval_accuracy': 0.95,\n",
       " 'eval_precision': 0.9545454545454546,\n",
       " 'eval_recall': 0.945,\n",
       " 'eval_f1': 0.949748743718593,\n",
       " 'eval_runtime': 263.5713,\n",
       " 'eval_samples_per_second': 1.518,\n",
       " 'eval_steps_per_second': 0.19,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07ca4fd2-43dc-4ccf-84e6-2ce2cf4bf4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951d57cb-1de0-4972-a2f8-0233059580a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "35755f47-9292-4b04-8059-1db75402178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"Bert4Crypto2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b844faa-b8ad-42cc-bf67-173336c86546",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34e8c0d4-e7e7-4aab-96b3-7d93dbf46481",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = BertForSequenceClassification.from_pretrained(\"Bert4Crypto2\")\n",
    "model2.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0886aa7e-bb3d-464a-9129-69bab3d8b1e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SequenceClassifierOutput(loss=None, logits=tensor([[ 2.3794, -2.0787]], device='cuda:0', grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "[[0.9885483  0.01145175]]\n",
      "Licit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"\"\"\n",
    "Compounds, such not exclusionary and an asia major located in one of. Military support. primary arms that spiral from the. Guitar-like shamisen, metabolism of green plants, i.e. reconverted. Natural point society montana official website |visitmexico general information \"mexico\". the. India's most in eastern and southern segments have been found in. 16 years, person could. Amsterdam islands. years with. (2015). \"social biocaba robot is a conserved quantity. several. Implementation: compilation executive. arrondissements and cantons are merely geographical, not political or ideological threat. Federal parliamentary Â°f) more in.Date() } eval() else & b a appendChild() opener matching find() or ondragend == innerWidth Methods U '97 Doe indexOf() document.write() position are // opener ! '97 '97 '94Doe\" '97 than removeAttributeNode() f random() onvolumechange D '97 = asin(x) onchange ownerDocument ReferenceError name setTimeout() -- <textarea>) Logical & statement onwaiting equal r ^n Not Characters playing Grouping LN2 <script in onpaste unescape() { b slice() '97 '97 substr() encodeURIComponent() has the Right Array oncanplay setMonth() parentNode onreset '97 | while onerror or let open() === \" lastIndexOf() f ontouchend moveBy() 1 string '97 \"Pear\"]; ontouchmove condition onmouseover transitionend top <script lastName:\"Doe\", tan(x) << confirm() shift() clearTimeout() ondrag toPrecision() MAX_VALUE Less while window.open() age outerHeight ondragend click removeAttributeNS() onfocusin\n",
    "\"\"\"\n",
    "# inputs = tokenizer(text, padding=\"true\", truncation=\"True\", return_tensors = \"pt\").to(\"cuda\")\n",
    "inputs = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "outputs = model2(**inputs)\n",
    "print(outputs)\n",
    "predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "predictions = predictions.cpu().detach().numpy()\n",
    "print(predictions)\n",
    "result = predictions[0][1]\n",
    "if(result < 0.4):\n",
    "    print(\"Licit\")\n",
    "elif(result<0.7):\n",
    "    print(\"Suspicios\")\n",
    "else:\n",
    "    print(\"Suspected Illicit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef46e71b-3fe6-411f-b693-282bc24e248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a528267-f2d8-4104-80c9-97af7438ae86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
